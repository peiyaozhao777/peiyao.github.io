<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8"/>
	<title>tree-risk-ai/</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- RSS feed -->
	<link rel="alternate" type="application/rss+xml" title="NARS Lab" href="http://localhost:4000/feed.xml">

    <!-- Customized Bootstrap + Font Awesome + Solarized -->
    <link href="/css/style.css" rel="stylesheet" media="screen">

	<!-- Favicon -->
	<link rel="shortcut icon" href="/images/favicon.png"/>

	<!-- Typekit -->
	<script>
	  (function(d) {
		var config = {
		  kitId: 'xeu8jut',
		  scriptTimeout: 3000
		},
		h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='//use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)
	  })(document);
	</script>

	<!-- Google Analytics -->
	<!-- <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-26244371-2', 'bedford.io');
		ga('send', 'pageview');

	</script> -->

	<!-- jQuery -->
	<script src="/js/jquery.min.js"></script>

	<!-- Bootstrap -->
	<script src="/js/transition.js"></script>
	<script src="/js/collapse.js"></script>

</head>

<body>

	<div id="header">
		<nav class="navbar navbar-default navbar-static-top" role="navigation">
			<div class="container">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-element" aria-expanded="false">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
					
					<object class="logo navbar-brand" data="/images/logo.svg" type="image/svg+xml"></object>
					<a class="navbar-brand" href="/">N A R S Lab</a>
					
				</div>
				<div class="collapse navbar-collapse" id="navbar-collapse-element">
					<ul class="nav navbar-nav navbar-right">
						
						<li>
						
						<a href="/blog/">Blog</a>
						</li>
						
						<li>
						
						<a href="/papers/">Publications</a>
						</li>
						
						<li>
						
						<a href="/projects/">Projects</a></li>
						
						<li>
						
						<a href="/courses/">Courses</a></li>
						
						<li>
						
						<a href="/team/">Team</a></li>
					</ul>
				</div>
			</div>
		</nav>
	</div>

	<div class="container">

	

<div class="row">
	<div class="col-md-12">
		<div class="bigtitle titlebox">
			<ol class="breadcrumb">
			
			
				
				
				
				<li>tree-risk-ai</li>
				
			
			</ol>
		</div>
		<p>
		<div class="head">
			Artificial Intelligence for Tree Failure Identification and Risk Quantification
		</div>
	</div>
</div>

<div class="bigspacer"></div>

<div class="row">
	<div class="col-md-3">
		<div class="bigspacer"></div>
		<div class="smallhead">
			Source code
		</div>
		<div class="pad-left note">
			<div class="smallspacer"></div>
			<i class="fa fa-cog fa-fw"></i>
			<a class="off" href="https://github.com/narslab/tree-risk-ai">github.com/narslab/<wbr>tree-risk-ai</a>
		</div>
		<div class="bigspacer"></div>
		<div class="smallhead">
			Contributors
		</div>
		<div class="pad-left note">
			
			<div class="smallspacer"></div>
			<div>
				<a class="off" href="https://github.com/jimioke">
    			<img class="pull-left avatar" src="https://avatars.githubusercontent.com/u/6685350?v=4&s=50">
    			<div class="handlebox" style="padding-left:5px;"">
    				jimioke
    			</div>
 				</a>
 			</div>
 			
			<div class="smallspacer"></div>
			<div>
				<a class="off" href="https://github.com/naskoap">
    			<img class="pull-left avatar" src="https://avatars.githubusercontent.com/u/10101246?v=4&s=50">
    			<div class="handlebox" style="padding-left:5px;"">
    				naskoap
    			</div>
 				</a>
 			</div>
 			
		</div>
		<div class="bigspacer"></div>
		<div class="smallhead">
			Latest commits
		</div>
		<div class="pad-left smallnote">
			<ul class="list-unstyled">
			
				<div class="smallspacer"></div>
				<li>
					<i class="fa fa-check-square-o fa-fw"></i>
					<a class="off" href="https://github.com/narslab/tree-risk-ai/commit/6cb1df60f12167059dcc3c5ee15625090255c4df">
					08 Mar 2021 - <span class="text-gray">Improved README documentation of all scripts and notebooks</span>
					</a>
				</li>
			
				<div class="smallspacer"></div>
				<li>
					<i class="fa fa-check-square-o fa-fw"></i>
					<a class="off" href="https://github.com/narslab/tree-risk-ai/commit/15913d60af6c3710a9f76a31e087d0f0ea093c32">
					08 Mar 2021 - <span class="text-gray">Created arXiv version of manuscript (relevant options added in documentclass declaration)</span>
					</a>
				</li>
			
				<div class="smallspacer"></div>
				<li>
					<i class="fa fa-check-square-o fa-fw"></i>
					<a class="off" href="https://github.com/narslab/tree-risk-ai/commit/4ae6513a3a3c063d60eaa5ddfd659d33f0492596">
					05 Mar 2021 - <span class="text-gray">doublespaced manuscript</span>
					</a>
				</li>
			
				<div class="smallspacer"></div>
				<li>
					<i class="fa fa-check-square-o fa-fw"></i>
					<a class="off" href="https://github.com/narslab/tree-risk-ai/commit/b70cd158f2ab484acc9e3f5e582a17203f3061bb">
					05 Mar 2021 - <span class="text-gray">final edits to first journal submission</span>
					</a>
				</li>
			
				<div class="smallspacer"></div>
				<li>
					<i class="fa fa-check-square-o fa-fw"></i>
					<a class="off" href="https://github.com/narslab/tree-risk-ai/commit/1b10e655d8e3fa4eb508430e2541a6c05e5f040b">
					05 Mar 2021 - <span class="text-gray">Merge remote-tracking branch &#39;refs/remotes/origin/master&#39;</span>
					</a>
				</li>
			
			</ul>
		</div>
		<div class="spacer"></div>

		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
			<!-- 19 -->
			
		
			
		
			
		

		
		<div class="smallhead">
			Pages
		</div>
		<div class="pad-left smallnote">
			<ul class="list-unstyled">
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
			
				
				<div class="smallspacer"></div>
				<li>
					<a class="off" href="/projects/tree-risk-ai/">
					<i class="fa fa-folder-open-o"></i>  / 
					</a>
				</li>
				
			
				
			
				
			
			</ul>
		</div>
		<div class="spacer"></div>
		

	</div>
	<div class="col-md-8">
		<div class="post">
			<p>
			<h1 id="tree-risk-ai">tree-risk-ai</h1>
<p>Artificial Intelligence for Tree Failure Identification and Risk Quantification</p>

<h2 id="summary">Summary</h2>
<p>The code and models in this repository implement convolutional neural network (CNN) models to predict tree likelihood of failure categories from a given input image. The categories are:</p>
<ul>
  <li>Improbable: failure unlikely either during normal or extreme weather conditions</li>
  <li>Possible: failure expected under extreme weather conditions; but unlikely during normal weather conditions</li>
  <li>Probable: failure expected under normal weather conditions within a given time frame
Original input images are 3024 x 4032 pixels. We assess the performance of an optimized CNN using 64-pixel, 128-pixel and 224-pixel inputs (after data augmentation expands samples from 525 images to 2525 images).
We also evaluate performance under four classification scenarios (investigating how various category groupings impact classifier performance):
Pr_Im: {Probable, Improbable}; 2 classes
PrPo_Im: {Probable + Possible, Improbable}; 2 classes
Pr_PoIm: {Probable, Possible + Improbable}; 2 classes
Pr_Po_Im: {Probable, Possible, Improbable}; 3 classes</li>
</ul>

<h2 id="step-1-label-input-data">Step 1: Label input data</h2>
<p>Inputs are images (currently 3024 x 4032 pixels). These are currently saved locally and not accessible on the remote. Email the collaborators for data access. To perform labeling, run <code class="language-plaintext highlighter-rouge">label-image-files.py</code>. The user must specify the path to the raw images (<code class="language-plaintext highlighter-rouge">RAW_IMAGE_DIR</code>). The current framework assumes the raw images are housed in <code class="language-plaintext highlighter-rouge">data/raw/Pictures for AI</code>.</p>

<h2 id="step-2-preprocess-images">Step 2: Preprocess images</h2>
<p>In this step (<code class="language-plaintext highlighter-rouge">preprocess-images.py</code>), we perform image resizing and data augmentation (random cropping, horizontal flipping - probability of 50%). The user can specify the expansion factor for the original set of images. For instance, there are 525 images in the original dataset. if an expansion factor of 5 is specified for the preprocessing function, then the final augmented set will contain 2525 images. Finally, image training sets are generated for 4 classification scenarios and for user-specified resolutions, e.g. 64 x 64 px, 128 x 128 px, etc. One-hot-vector encoding is also performed. Each set of images and labels are saved as an array of tuples in a binary <code class="language-plaintext highlighter-rouge">.npy</code> file. The <code class="language-plaintext highlighter-rouge">preprocess-images.py</code> script also includes a <code class="language-plaintext highlighter-rouge">plotProcessedImages()</code> function that generates a specified number of randomly chosen input images for each scenario.</p>

<p>The user can also plot selected processed images using the functions in the <code class="language-plaintext highlighter-rouge">plot-processed-images.py</code> script. To explore all the processed images in a matrix plot, use <code class="language-plaintext highlighter-rouge">exploreProcessedImages()</code>. Figure 2 in the manuscript was generated using the <code class="language-plaintext highlighter-rouge">plotSelectedProcessedImages()</code> function.</p>

<h2 id="step-3-cnn-hyperparameter-optimization">Step 3: CNN hyperparameter optimization</h2>
<p>We use the <code class="language-plaintext highlighter-rouge">Hyperband</code> function from <code class="language-plaintext highlighter-rouge">keras.tuner</code> to optimize the following parameters in our convolutional neural network: kernel size of first convolutional layer, units in the 2 dense layers, their respective dropout rates and activation functions. The routine is carried out in <code class="language-plaintext highlighter-rouge">cnn-hyperparameter-optimization.py</code>. The search is performed for 12 cases (3 resolutions and 4 classification scenarios).</p>
<ul>
  <li>The results are tabulated via <code class="language-plaintext highlighter-rouge">tabulate-optimal-hyperparameters.py</code> (which generates the CSV files used to create Table 4 in the manuscript).</li>
</ul>

<h2 id="step-4-sensitivity-tests">Step 4: Sensitivity tests</h2>
<p>In <code class="language-plaintext highlighter-rouge">resolution-scenario-sensitivity.py</code>, the function <code class="language-plaintext highlighter-rouge">testResolutionScenarioPerformance()</code> conducts CNN model fitting for each combination of resolution and scenario as specified by the user in <code class="language-plaintext highlighter-rouge">RESOLUTION_LIST</code> and <code class="language-plaintext highlighter-rouge">SCENARIO_LIST</code> respectively. This is done via k-fold cross-validation. Validation metrics of macro-average precision, recall and $F_1$ are also implemented. Model histories are saved for each trial.</p>

<p>Tabulation and visualization summaries of the results are implemented in <code class="language-plaintext highlighter-rouge">senstivity-analysis.ipynb</code>.</p>
<ul>
  <li>Figure 4 in the manuscript is generated using <code class="language-plaintext highlighter-rouge">plotMeanAccuracyLoss()</code>.</li>
  <li>Figure 5 is generated using <code class="language-plaintext highlighter-rouge">plotSummaryValidationMetrics()</code></li>
</ul>

<p>Furthermore, we aggregate performance statistics in <code class="language-plaintext highlighter-rouge">senstivity-analysis.ipynb</code> and performance Welchâ€™s tests to determine  if there are significant differences in outcomes.</p>
<ul>
  <li>The function <code class="language-plaintext highlighter-rouge">getScenarioResolutionMeanPerformance()</code> generates Table 6.</li>
  <li>The function <code class="language-plaintext highlighter-rouge">resolutionPerformanceComparisonStats()</code> generates Table 7.</li>
  <li>The function <code class="language-plaintext highlighter-rouge">scenarioPerformanceComparisonStats()</code> generates Table 8.</li>
</ul>

<h2 id="step-5-detailed-cnn-performance-analysis">Step 5: Detailed CNN performance analysis</h2>
<p>In <code class="language-plaintext highlighter-rouge">cnn-performance.py</code>, we define the function <code class="language-plaintext highlighter-rouge">trainModelWithDetailedMetrics()</code> which implements CNN model-fitting, along with sklearn classification metrics, including a confusion matrix, for a given resolution/scenario instance. The loss and performance results are visualized in the <code class="language-plaintext highlighter-rouge">plot-cnn-performance.ipynb</code> notebook, using the function <code class="language-plaintext highlighter-rouge">plotCNNPerformanceMetrics()</code>.</p>
<ul>
  <li>Figure 6 in the manuscript is generated via <code class="language-plaintext highlighter-rouge">plotCNNPerformanceMetrics()</code>.</li>
  <li>Figure 7 is based on the confusion matrices saved from running <code class="language-plaintext highlighter-rouge">getScenarioModelPerformance()</code>, which in turns runs <code class="language-plaintext highlighter-rouge">trainModelWithDetailedMetrics()</code>.
The trained model is saved to <code class="language-plaintext highlighter-rouge">results/models/</code>.</li>
</ul>

<h2 id="step-6-cnn-visualization-and-inference-in-progress">Step 6: CNN Visualization and Inference (in progress)</h2>
<p>We implement GradCAM and saliency maps to understand how the CNN classifies an image. This is done using <code class="language-plaintext highlighter-rouge">plotGradCAM()</code> and <code class="language-plaintext highlighter-rouge">plotSaliency()</code> in <code class="language-plaintext highlighter-rouge">cnn-visualization.ipynb</code>. A prior trained model is loaded (e.g. <code class="language-plaintext highlighter-rouge">m = models.load_model('../../results/models/opt-cnn-Pr_Im-128-px/model')</code>) and used as an input to either of the functions mentioned.</p>

<p>Please note: Function and classes that are used in two or more scripts are housed in <code class="language-plaintext highlighter-rouge">helpers.py</code></p>

		</div>
	</div>
	<div class="col-md-1"></div>
</div>


	</div>

	<div id="footer"><span style="display:none">foo</span></div>

</body>
</html>
